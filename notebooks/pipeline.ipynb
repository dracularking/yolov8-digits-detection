{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87d49b52-5ac0-402a-84fb-7ac97803d6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "25968de8-1da3-4fe4-93a8-9b980ff74891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shate/.cache/pypoetry/virtualenvs/digits-detection-hppNHGvS-py3.11/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:65: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from src.onnx_model import YoloOnnxModel\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "from torch import nn\n",
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import onnxruntime as ort\n",
    "import torchvision\n",
    "from src.utils.vision import non_maximum_supression\n",
    "from src.visualization import plot_yolo_labels\n",
    "from src.transforms import xywh2xyxy, xywhn2xywh\n",
    "\n",
    "onnx_model = YoloOnnxModel(\"../models/detection_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dff7ee-1d73-4306-8e92-d880f49764f9",
   "metadata": {},
   "source": [
    "---\n",
    "# **yolo outputs to nms ONNX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "e2592517-cefa-4ccb-b426-f9aed197a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs_transposed = outputs.transpose(0, 2, 1)          # V\n",
    "\n",
    "# boxes_xywh = outputs_transposed[..., :4]      # V\n",
    "# class_scores = outputs_transposed[..., 4:]    # V\n",
    "\n",
    "# scores = class_scores.max(axis=2, keepdims=True) # V\n",
    "# scores = scores.transpose(0, 2, 1)  # V\n",
    "\n",
    "# class_ids = class_scores.argmax(axis=2).squeeze()\n",
    "\n",
    "# iou_threshold = np.array([0.7], dtype=np.float32)\n",
    "# score_threshold = np.array([0.25], dtype=np.float32)\n",
    "\n",
    "# pad_x, pad_y = pad\n",
    "# w = onnx_model.input_w - pad_x * 2\n",
    "# h = onnx_model.input_h - pad_y * 2\n",
    "\n",
    "# boxes_xywh[:, :, 0] -= pad_x\n",
    "# boxes_xywh[:, :, 1] -= pad_y\n",
    "\n",
    "# boxes_xywhn = np.divide(boxes_xywh, np.array([w, h, w, h]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "82adf626-50fd-4f16-af54-9acb2322df89",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"../datasets/yolo_HWD+/images/test/42.png\")\n",
    "# image = Image.open(\"../datasets/SVHN/examples/5.png\")\n",
    "\n",
    "image_ = np.asarray(image)\n",
    "image_ = image_[100:-50, 40:-50]\n",
    "img = np.copy(image_)\n",
    "\n",
    "input_tensor, pad = onnx_model.prepare_input(img)\n",
    "outputs = onnx_model.inference(input_tensor)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "332a3b81-6369-404e-8d97-507a27d6a0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[      186.4,      62.237,      29.219,      59.791],\n",
       "        [      62.26,      186.25,      37.847,      60.953],\n",
       "        [     215.28,      111.26,      26.218,       59.98]], dtype=float32),\n",
       " array([    0.99841,     0.99451,     0.86349], dtype=float32),\n",
       " array([9, 5, 0], dtype=int32)]"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.onnx_nms import create_onnx_NMS\n",
    "\n",
    "create_onnx_NMS(filepath=\"nms.onnx\", opset=18)\n",
    "nms = ort.InferenceSession(\"nms.onnx\", providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
    "\n",
    "CONF_THRESHOLD = 0.25\n",
    "IOU_THRESHOLD = 0.7\n",
    "MAX_OUTPUT_BOXES_PER_CLASS = 100\n",
    "\n",
    "nms_results = nms.run(\n",
    "    [\"selected_boxes_xywh\", \"selected_class_scores\", \"selected_class_ids\"], \n",
    "    {\n",
    "        \"output0\": outputs, \n",
    "        \"max_output_boxes_per_class\": np.array([MAX_OUTPUT_BOXES_PER_CLASS], dtype=np.int32),\n",
    "        'iou_threshold': np.array([IOU_THRESHOLD], dtype=np.float32), \n",
    "        'score_threshold': np.array([CONF_THRESHOLD], dtype=np.float32)\n",
    "    }\n",
    ")\n",
    "nms_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d03b688-2fae-4591-9e5a-558988254b44",
   "metadata": {},
   "source": [
    "---\n",
    "# **preprocessing onnx**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "be0e0745-3b68-49c8-b08a-f62b18de6c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_h, input_w, input_c = 256, 256, 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "5a0b2e71-1c04-43ea-bf4e-9a4a7d1e8bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = img[..., :input_c]    # V\n",
    "\n",
    "# img_h, img_w, img_c = img.shape   # V\n",
    "# aspect_ratio = img_w / img_h  # V\n",
    "# if aspect_ratio > 1:  # V\n",
    "#     new_img_h = int(input_w / aspect_ratio)  # V\n",
    "#     new_img_w = input_w  # V\n",
    "# else:  # V\n",
    "#     new_img_h = input_h  # V\n",
    "#     new_img_w = int(input_h / aspect_ratio)  # V\n",
    "    \n",
    "# resized_img = cv2.resize(img, (new_img_w, new_img_h), interpolation=cv2.INTER_LINEAR)  # V\n",
    "\n",
    "# # width, height ratios\n",
    "# padded_img = np.ones((input_h, input_w, img_c)) * fill_value\n",
    "# left = (input_w - new_img_w) // 2\n",
    "# bottom = (input_h - new_img_h) // 2\n",
    "\n",
    "# padded_img[bottom : bottom + new_img_h, left : left + new_img_w] = resized_img\n",
    "# pad_x = (input_w - new_img_w) // 2\n",
    "# pad_y = (input_h - new_img_h) // 2\n",
    "\n",
    "# pad = (pad_x, pad_y)\n",
    "# img = padded_img\n",
    "\n",
    "# img = img / 255.0\n",
    "# img = img.transpose(2, 0, 1)\n",
    "# input_tensor = img.astype(np.float32)\n",
    "# # input_tensor = np.expand_dims(img, 0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "3b3d725f-1f62-4c5a-b553-ab8a0e995cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.onnx_preprocessing import create_onnx_preprocessing\n",
    "\n",
    "create_onnx_preprocessing(\"preprocessing.onnx\")\n",
    "preprocessing = ort.InferenceSession(\"preprocessing.onnx\", providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "46c060b3-fe20-48d1-a2e5-a0cd14ad4644",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 21:50:37.590253853 [W:onnxruntime:, execution_frame.cc:857 VerifyOutputSizes] Expected shape from model of {3,-1,3} does not match actual shape of {3,256,256} for output preprocessed_img\n"
     ]
    }
   ],
   "source": [
    "INPUT_H = 256\n",
    "INPUT_W = 256\n",
    "FILL_VALUE = 114\n",
    "\n",
    "inputs = {\n",
    "    \"image\": image_, \n",
    "    \"input_h\": np.array([INPUT_H], dtype=np.int32), \n",
    "    \"input_w\": np.array([INPUT_W], dtype=np.int32),\n",
    "    \"fill_value\": np.array([FILL_VALUE], dtype=np.uint8)\n",
    "}\n",
    "preprocessed = preprocessing.run(None, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "ded30ec9-895d-4293-87a5-e68ebd392f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 256, 256)"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "62350760-44bc-4d21-b877-14dfbdd0f0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14,  0, 14,  0], dtype=int32)"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "73d0730e-cc05-4d26-a8f5-87b5967b3411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470cbcac-0b05-46e0-a30b-e7935838c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit onnx_model(img, 0.7, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffd7997-7d78-4a25-82a7-662efe561eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digits-detection-kernel",
   "language": "python",
   "name": "digits-detection-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
